# =============================================================================
# API Keys for LLM Services
# Copy this file to .env and fill in your actual API keys
# =============================================================================

# OpenAI API Key
# REQUIRED for: DarkBench secondary behavior evaluation (5 datasets: Brand Bias,
#               Sycophancy, Anthropomorphism, User Retention, Sneaking)
#               Uses GPT-4o for dark pattern detection
OPENAI_API_KEY=your_openai_api_key_here

# Groq API Key
# REQUIRED for: Default intrinsic hallucination evaluation (llama-3.3-70b-versatile)
# OPTIONAL: Fast Llama model inference, refusal evaluation (if REFUSAL_EVAL_METHOD=GROQ)
GROQ_API_KEY=your_groq_api_key_here

# Together AI API Key
# OPTIONAL: Alternative for refusal evaluation (if REFUSAL_EVAL_METHOD=TOGETHER)
# OPTIONAL: Alternative for hallucination evaluation
TOGETHER_API_KEY=your_together_api_key_here

# Llama API Key
# OPTIONAL: For using Meta's Llama API directly (if REFUSAL_EVAL_METHOD=LLAMA)
LLAMA_API_KEY=your_llama_api_key_here

# VLLM Configuration (if using REFUSAL_EVAL_METHOD=VLLM):
# OPTIONAL: For local refusal evaluation using vLLM serving Llama Guard 4
VLLM_BASE_URL=http://localhost:8000/v1
VLLM_API_KEY=dummy

# VLLM Instructions:
# If running refusal evaluation with VLLM, to replicate the paper setup,
# first serve Llama Guard 4-12B locally with the following command:
# HF_TOKEN=xxx vllm serve meta-llama/Llama-Guard-4-12B --port 8000 --max-model-len 10000
# on a GPU via vLLM. 
# In the paper it was run in a separate env with vLLM 0.9.0 and "transformers<4.54.0"

# =============================================================================
# Evaluation Configuration
# =============================================================================

# Refusal Evaluation Method
# Controls which service evaluates harmful content using Llama Guard 4
# Options: VLLM (local, recommended, default used in paper), GROQ (fast but rate-limited), TOGETHER, LLAMA
REFUSAL_EVAL_METHOD=GROQ

# Intrinsic Hallucination Evaluation Model
# Which model judges factual accuracy for hallucination datasets
# Options:
#   - llama-3.3-70b-versatile (via Groq, default used in paper)
#   - Llama-3.3-70B-Instruct (via Together/Llama API)
INTRINSIC_HALLUCINATION_MODEL=llama-3.3-70b-versatile
